{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5483028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "sb.set()\n",
    "b = pd.read_csv('data 2.csv')\n",
    "b = b.drop(columns=['id', 'Unnamed: 32'])\n",
    "b['diagnosis'] = b['diagnosis'].replace([\"M\", \"B\"], [1, 0])\n",
    "y = b[\"diagnosis\"] #Predictors\n",
    "X = b.drop(\"diagnosis\", axis=1) #Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99974e0",
   "metadata": {},
   "source": [
    "# Core Analysis using Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d0574",
   "metadata": {},
   "source": [
    "Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9fc38",
   "metadata": {},
   "source": [
    "The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n",
    "\n",
    "We have tried to predict using linear polynomial and gaussian kernels to predict which one would be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec5f1c",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "244b30eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58  2]\n",
      " [ 5 49]]\n",
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 69) #Ensure same random state in every model to ensure comparison is fair\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1bbab",
   "metadata": {},
   "source": [
    "## Polynomial Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1460653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58  2]\n",
      " [13 41]]\n",
      "0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=8)\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b74a1",
   "metadata": {},
   "source": [
    "## Gaussian Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c6ea08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  1]\n",
      " [ 4 17]]\n",
      "0.9107142857142857\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908cf5c",
   "metadata": {},
   "source": [
    "## Linear and Gaussian SVM using Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5082b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy scores for each fold: [0.9298245614035088, 0.9298245614035088, 0.9649122807017544, 0.8421052631578947, 0.9298245614035088, 0.8771929824561403, 0.9473684210526315, 0.9824561403508771, 0.9473684210526315, 0.9464285714285714]\n",
      "Gaussian SVM accuracy scores for each fold: [0.9298245614035088, 0.9122807017543859, 0.9649122807017544, 0.8421052631578947, 0.9298245614035088, 0.8771929824561403, 0.9473684210526315, 0.9824561403508771, 0.9473684210526315, 0.9285714285714286]\n",
      "Linear SVM with Regularisation accuracy scores for each fold: [0.9298245614035088, 0.9298245614035088, 0.9649122807017544, 0.8421052631578947, 0.9298245614035088, 0.8771929824561403, 0.9649122807017544, 0.9824561403508771, 0.9473684210526315, 0.9464285714285714]\n",
      "Average accuracy score for Linear SVM: 0.9297305764411027\n",
      "Average accuracy score for Gaussian SVM: 0.9261904761904761\n",
      "Average accuracy score for Linear SVM with regularisation: 0.931484962406015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming X and y are already defined and preprocessed\n",
    "\n",
    "# Performing PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Defining the number of folds for stratified k-fold cross-validation\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "\n",
    "# Initializing the linear SVM classifier\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svmC = SVC(kernel='linear', random_state=42, C=12)\n",
    "Gsvm = SVC(kernel='rbf', random_state=42, C=12)\n",
    "\n",
    "accuracy_scores_svm = []\n",
    "accuracy_scores_Gsvm = []\n",
    "accuracy_scores_svmC = []\n",
    "\n",
    "# Performing stratified k-fold cross-validation\n",
    "for train_index, test_index in skf.split(X_pca, y):\n",
    "    X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Linear SVM\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "    accuracy_scores_svm.append(accuracy_svm)\n",
    "    \n",
    "    # Linear SVM with Regularisation\n",
    "    svmC.fit(X_train, y_train)\n",
    "    y_pred_svmC = svmC.predict(X_test)\n",
    "    accuracy_svmC = accuracy_score(y_test, y_pred_svmC)\n",
    "    accuracy_scores_svmC.append(accuracy_svmC)\n",
    "    \n",
    "    #Gaussian SVM\n",
    "    Gsvm.fit(X_train, y_train)\n",
    "    y_pred_Gsvm = Gsvm.predict(X_test)\n",
    "    accuracy_Gsvm = accuracy_score(y_test, y_pred_Gsvm)\n",
    "    accuracy_scores_Gsvm.append(accuracy_Gsvm)\n",
    "    \n",
    "\n",
    "# Printing the accuracy scores for each fold\n",
    "print(\"Linear SVM accuracy scores for each fold:\", accuracy_scores_svm)\n",
    "print(\"Gaussian SVM accuracy scores for each fold:\", accuracy_scores_Gsvm)\n",
    "print(\"Linear SVM with Regularisation accuracy scores for each fold:\", accuracy_scores_svmC)\n",
    "\n",
    "# Calculating the average accuracy score\n",
    "average_accuracy_svm = np.mean(accuracy_scores_svm)\n",
    "print(\"Average accuracy score for Linear SVM:\", average_accuracy_svm)\n",
    "average_accuracy_Gsvm = np.mean(accuracy_scores_Gsvm)\n",
    "print(\"Average accuracy score for Gaussian SVM:\", average_accuracy_Gsvm)\n",
    "average_accuracy_svmC = np.mean(accuracy_scores_svmC)\n",
    "print(\"Average accuracy score for Linear SVM with regularisation:\", average_accuracy_svmC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed9b4a",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "SVM Classifiers offer good accuracy and perform faster prediction  They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "SVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Na√Øve Bayes. It works poorly with overlapping classes and is also sensitive to the type of kernel used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082cd73",
   "metadata": {},
   "source": [
    "### Why did we add a regularization parameter (C=)?\n",
    "\n",
    "Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
    "\n",
    "\n",
    "Through trial and error, we have found that C=12 gives the highest prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b63af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
